"""
YAMNet TFLite ì¶”ë¡  ëª¨ë“ˆ - ë¼ì¦ˆë² ë¦¬íŒŒì´ìš© (í†µí•© ëª¨ë¸)
audio_client.pyì™€ í†µí•© ê°€ëŠ¥ (bytes ì…ë ¥ ì§€ì›)
48kHz â†’ 16kHz ë¦¬ìƒ˜í”Œë§ ì§€ì›
YAMNet + Classifier í†µí•© ëª¨ë¸ ì‚¬ìš© (ì›ìŠ¤í… ì¶”ë¡ )
"""
import numpy as np
import os
from scipy import signal

try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    import tensorflow.lite as tflite

CLASSIFIER_PATH = "models/classifier.tflite"  

SAMPLE_RATE = 16000
INPUT_SIZE = 15600  

CLASS_NAMES = [
    'Air_conditioner',      
    'Hair_dryer',           
    'Microwave',            
    'Others',               
    'Refrigerator_Hum',     
    'Vacuum'                
]


def resample_to_16k(audio_bytes, source_rate=48000):
    """
    48kHz -> 16kHz ë¦¬ìƒ˜í”Œë§

    Args:
        audio_bytes: int16 PCM bytes
        source_rate: ì›ë³¸ ìƒ˜í”Œë ˆì´íŠ¸ (ê¸°ë³¸ 48000)

    Returns:
        int16 PCM bytes (16kHz)
    """
    if source_rate == 16000:
        return audio_bytes  

    audio_np = np.frombuffer(audio_bytes, dtype=np.int16)

    num_samples_16k = int(len(audio_np) * 16000 / source_rate)
    audio_16k = signal.resample(audio_np, num_samples_16k)

    audio_16k = np.clip(audio_16k, -32768, 32767).astype(np.int16)

    return audio_16k.tobytes()


class YAMNetClassifier:
    """
    YAMNet + ë¶„ë¥˜ê¸° í†µí•© TFLite ëª¨ë¸
    audio_client.pyì—ì„œ ì‚¬ìš© ê°€ëŠ¥ (bytes ì…ë ¥ ì§€ì›)
    """

    def __init__(self, classifier_path=CLASSIFIER_PATH):
        """í†µí•© ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(classifier_path):
            raise FileNotFoundError(f"í†µí•© ëª¨ë¸ ì—†ìŒ: {classifier_path}")

        self.classifier = tflite.Interpreter(model_path=classifier_path)
        self.classifier.allocate_tensors()

        
        self.input_details = self.classifier.get_input_details()
        self.output_details = self.classifier.get_output_details()

        self.input_index = self.input_details[0]['index']
        self.output_index = self.output_details[0]['index']

        print(f"í†µí•© ëª¨ë¸ ë¡œë“œ: {classifier_path}")
        print(f"   ì…ë ¥ shape: {self.input_details[0]['shape']}")
        print(f"   ì¶œë ¥ shape: {self.output_details[0]['shape']}")

    def predict_from_bytes(self, audio_bytes, source_rate=48000):
        """
        ì‹¤ì‹œê°„ ìº¡ì²˜ëœ bytesë¥¼ ì¶”ë¡  (audio_client.pyìš©)
        ìë™ìœ¼ë¡œ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ ì§€ì›
        í†µí•© ëª¨ë¸ë¡œ ì›ìŠ¤í… ì¶”ë¡ 

        Args:
            audio_bytes: int16 PCM bytes
            source_rate: ì›ë³¸ ìƒ˜í”Œë ˆì´íŠ¸ (ê¸°ë³¸ 48000)

        Returns:
            predicted_class: 0~5 (í´ë˜ìŠ¤ ì¸ë±ìŠ¤)
            confidence: 0.0~1.0 (ì‹ ë¢°ë„)
        """
        
        if source_rate != 16000:
            audio_bytes = resample_to_16k(audio_bytes, source_rate)

        audio_np = np.frombuffer(audio_bytes, dtype=np.int16)

        audio_np = audio_np[:INPUT_SIZE]

        if len(audio_np) < INPUT_SIZE:
            audio_np = np.pad(audio_np, (0, INPUT_SIZE - len(audio_np)), 'constant')

        audio_float = audio_np.astype(np.float32) / 32768.0

        input_tensor = audio_float[np.newaxis, ...]

        self.classifier.set_tensor(self.input_index, input_tensor)
        self.classifier.invoke()
        probs = self.classifier.get_tensor(self.output_index)[0]  

        predicted_class = int(np.argmax(probs))
        confidence = float(probs[predicted_class])

        return predicted_class, confidence

    def classify_buffer(self, buffer_of_chunks, consistency_threshold=4, source_rate=48000, target_class=None):
        """
        5ê°œ ì²­í¬ ë²„í¼ ë¶„ë¥˜ (ì¼ê´€ì„± ì²´í¬)

        Args:
            buffer_of_chunks: 5ê°œ ì˜¤ë””ì˜¤ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (bytes)
            consistency_threshold: ì¼ê´€ì„± ì„ê³„ê°’ (ê¸°ë³¸ 4/5 = 80%)
            source_rate: ì›ë³¸ ìƒ˜í”Œë ˆì´íŠ¸ (ê¸°ë³¸ 48000)

        Returns:
            - "APPLIANCE_DETECTED": ê°€ì „ ì†ŒìŒ í™•ì¸
            - None: ì™¸ë¶€ ì†ŒìŒ ë˜ëŠ” ì¼ê´€ì„± ë¶€ì¡±
        """
        if len(buffer_of_chunks) != 5:
            print(f"âš ï¸ ì²­í¬ ê°œìˆ˜ ì˜¤ë¥˜: {len(buffer_of_chunks)} (5ê°œ í•„ìš”)")
            return None

        
        predictions = []
        confidences = []

        for i, chunk in enumerate(buffer_of_chunks):
            pred_class, confidence = self.predict_from_bytes(chunk, source_rate)
            predictions.append(pred_class)
            confidences.append(confidence)
            print(f"  Chunk {i+1}: {CLASS_NAMES[pred_class]} ({confidence*100:.1f}%)")

        
        unique, counts = np.unique(predictions, return_counts=True)
        most_common_class = int(unique[np.argmax(counts)])
        most_common_count = int(np.max(counts))

        print(f"ê²°ê³¼: {CLASS_NAMES[most_common_class]} ({most_common_count}/5 ì¼ì¹˜)")

        
        if most_common_class == 3:
            print("ì™¸ë¶€ ì†ŒìŒ (Others) - ë¬´ì‹œ")
            return None

        
        if most_common_count >= consistency_threshold:
            
            if target_class is not None and most_common_class != target_class:
                print(f"ë‹¤ë¥¸ ê°€ì „ ì†ŒìŒ ({CLASS_NAMES[most_common_class]}) - ë¬´ì‹œ (target: {CLASS_NAMES[target_class] if target_class < len(CLASS_NAMES) else target_class})")
                return None
            print(f"ê°€ì „ ì†ŒìŒ í™•ì¸: {CLASS_NAMES[most_common_class]}")
            return "APPLIANCE_DETECTED"
        else:
            print(f"ì¼ê´€ì„± ë¶€ì¡± ({most_common_count}/5) - ë¬´ì‹œ")
            return None


def load_wav_16k_mono(filename):
    """
    í…ŒìŠ¤íŠ¸ìš©: wav íŒŒì¼
    """
    import wave

    try:
        with wave.open(filename, 'rb') as wf:
            if wf.getframerate() != 16000:
                print(f"âš ï¸ {filename}ì€ 16kHzê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return np.zeros(INPUT_SIZE, dtype=np.float32)

            if wf.getnchannels() != 1:
                print(f"âš ï¸ {filename}ì€ Monoê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return np.zeros(INPUT_SIZE, dtype=np.float32)

            frames = wf.readframes(wf.getnframes())
            audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0

            if len(audio_data) < INPUT_SIZE:
                audio_data = np.pad(audio_data, (0, INPUT_SIZE - len(audio_data)))
            else:
                audio_data = audio_data[:INPUT_SIZE]

            return audio_data

    except Exception as e:
        print(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return np.zeros(INPUT_SIZE, dtype=np.float32)


if __name__ == "__main__":
    print("YAMNet í†µí•© ëª¨ë¸ í…ŒìŠ¤íŠ¸...")

    try:
        
        classifier = YAMNetClassifier()

        
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ 1: ë”ë¯¸ ì˜¤ë””ì˜¤ 48kHz (bytes) â†’ 16kHz ë¦¬ìƒ˜í”Œë§")
        dummy_audio_48k = np.random.randint(-32768, 32767, 48000, dtype=np.int16).tobytes()
        pred_class, confidence = classifier.predict_from_bytes(dummy_audio_48k, source_rate=48000)
        print(f"   ê²°ê³¼: {CLASS_NAMES[pred_class]} ({confidence*100:.1f}%)")

        
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ 2: 5ê°œ ì²­í¬ ë²„í¼ (48kHz)")
        buffer = [dummy_audio_48k] * 5
        result = classifier.classify_buffer(buffer, source_rate=48000)
        print(f"   ìµœì¢… ê²°ê³¼: {result}")

        
        test_file = "test.wav"
        if os.path.exists(test_file):
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ 3: {test_file}")
            audio_float = load_wav_16k_mono(test_file)
            
            audio_bytes = (audio_float * 32768).astype(np.int16).tobytes()
            pred_class, confidence = classifier.predict_from_bytes(audio_bytes, source_rate=16000)
            print(f"   ê²°ê³¼: {CLASS_NAMES[pred_class]} ({confidence*100:.1f}%)")

        print("\nëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ğŸ“ í†µí•© ëª¨ë¸ (YAMNet + Classifier) ì •ìƒ ì‘ë™")

    except Exception as e:
        print(f"ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
