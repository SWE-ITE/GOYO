"""
YAMNet TFLite ì¶”ë¡  ëª¨ë“ˆ - ë¼ì¦ˆë² ë¦¬íŒŒì´ìš© (í†µí•© ëª¨ë¸)
audio_client.pyì™€ í†µí•© ê°€ëŠ¥ (bytes ì…ë ¥ ì§€ì›)
48kHz â†’ 16kHz ë¦¬ìƒ˜í”Œë§ ì§€ì›
YAMNet + Classifier í†µí•© ëª¨ë¸ ì‚¬ìš© (ì›ìŠ¤í… ì¶”ë¡ )
"""
import numpy as np
import os
from scipy import signal

# ë¼ì¦ˆë² ë¦¬ íŒŒì´ìš© tflite
try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    import tensorflow.lite as tflite

# ================= ì„¤ì • =================
CLASSIFIER_PATH = "models/classifier.tflite"  # í†µí•© ëª¨ë¸ (YAMNet + Classifier)

SAMPLE_RATE = 16000
INPUT_SIZE = 15600  # YAMNet ì…ë ¥ ìƒ˜í”Œ ê°œìˆ˜

CLASS_NAMES = [
    'Air_conditioner',      # 0
    'Hair_dryer',           # 1
    'Microwave',            # 2
    'Others',               # 3
    'Refrigerator_Hum',     # 4
    'Vacuum'                # 5
]


# ================= ë¦¬ìƒ˜í”Œë§ í•¨ìˆ˜ =================
def resample_to_16k(audio_bytes, source_rate=48000):
    """
    48kHz (ë˜ëŠ” ë‹¤ë¥¸ ìƒ˜í”Œë ˆì´íŠ¸) â†’ 16kHz ë¦¬ìƒ˜í”Œë§

    Args:
        audio_bytes: int16 PCM bytes
        source_rate: ì›ë³¸ ìƒ˜í”Œë ˆì´íŠ¸ (ê¸°ë³¸ 48000)

    Returns:
        int16 PCM bytes (16kHz)
    """
    if source_rate == 16000:
        return audio_bytes  # ì´ë¯¸ 16kHzë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # bytes â†’ numpy int16
    audio_np = np.frombuffer(audio_bytes, dtype=np.int16)

    # ë¦¬ìƒ˜í”Œ (source_rate â†’ 16000)
    num_samples_16k = int(len(audio_np) * 16000 / source_rate)
    audio_16k = signal.resample(audio_np, num_samples_16k)

    # int16ë¡œ ë³€í™˜
    audio_16k = np.clip(audio_16k, -32768, 32767).astype(np.int16)

    return audio_16k.tobytes()


# ================= ì¶”ë¡  í´ë˜ìŠ¤ (audio_client.pyìš©) =================
class YAMNetClassifier:
    """
    YAMNet + ë¶„ë¥˜ê¸° í†µí•© TFLite ëª¨ë¸
    audio_client.pyì—ì„œ ì‚¬ìš© ê°€ëŠ¥ (bytes ì…ë ¥ ì§€ì›)
    """

    def __init__(self, classifier_path=CLASSIFIER_PATH):
        """í†µí•© ëª¨ë¸ ë¡œë“œ (YAMNet + Classifier ì›ìŠ¤í…)"""
        if not os.path.exists(classifier_path):
            raise FileNotFoundError(f"âŒ í†µí•© ëª¨ë¸ ì—†ìŒ: {classifier_path}")

        self.classifier = tflite.Interpreter(model_path=classifier_path)
        self.classifier.allocate_tensors()

        # ì…ë ¥/ì¶œë ¥ í…ì„œ ì¸ë±ìŠ¤
        self.input_details = self.classifier.get_input_details()
        self.output_details = self.classifier.get_output_details()

        self.input_index = self.input_details[0]['index']
        self.output_index = self.output_details[0]['index']

        print(f"âœ… í†µí•© ëª¨ë¸ ë¡œë“œ: {classifier_path}")
        print(f"   ì…ë ¥ shape: {self.input_details[0]['shape']}")
        print(f"   ì¶œë ¥ shape: {self.output_details[0]['shape']}")

    def predict_from_bytes(self, audio_bytes, source_rate=48000):
        """
        ì‹¤ì‹œê°„ ìº¡ì²˜ëœ bytesë¥¼ ì¶”ë¡  (audio_client.pyìš©)
        ìë™ìœ¼ë¡œ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ ì§€ì›
        í†µí•© ëª¨ë¸ë¡œ ì›ìŠ¤í… ì¶”ë¡ 

        Args:
            audio_bytes: int16 PCM bytes
            source_rate: ì›ë³¸ ìƒ˜í”Œë ˆì´íŠ¸ (ê¸°ë³¸ 48000)

        Returns:
            predicted_class: 0~5 (í´ë˜ìŠ¤ ì¸ë±ìŠ¤)
            confidence: 0.0~1.0 (ì‹ ë¢°ë„)
        """
        # 48kHz â†’ 16kHz ë¦¬ìƒ˜í”Œë§ (í•„ìš”ì‹œ)
        if source_rate != 16000:
            audio_bytes = resample_to_16k(audio_bytes, source_rate)

        # bytes â†’ numpy int16
        audio_np = np.frombuffer(audio_bytes, dtype=np.int16)

        # 15600 ìƒ˜í”Œë§Œ ì‚¬ìš©
        audio_np = audio_np[:INPUT_SIZE]

        # ë¶€ì¡±í•œ ìƒ˜í”Œ íŒ¨ë”©
        if len(audio_np) < INPUT_SIZE:
            audio_np = np.pad(audio_np, (0, INPUT_SIZE - len(audio_np)), 'constant')

        # float32 ì •ê·œí™” [-1.0, 1.0]
        audio_float = audio_np.astype(np.float32) / 32768.0

        # ì…ë ¥ shape: (1, 15600)
        input_tensor = audio_float[np.newaxis, ...]

        # í†µí•© ëª¨ë¸ ì¶”ë¡  (YAMNet + Classifier ì›ìŠ¤í…)
        self.classifier.set_tensor(self.input_index, input_tensor)
        self.classifier.invoke()
        probs = self.classifier.get_tensor(self.output_index)[0]  # (6,)

        # ê²°ê³¼
        predicted_class = int(np.argmax(probs))
        confidence = float(probs[predicted_class])

        return predicted_class, confidence

    def classify_buffer(self, buffer_of_chunks, consistency_threshold=4, source_rate=48000, target_class=None):
        """
        5ê°œ ì²­í¬ ë²„í¼ ë¶„ë¥˜ (ì¼ê´€ì„± ì²´í¬)

        Args:
            buffer_of_chunks: 5ê°œ ì˜¤ë””ì˜¤ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (bytes)
            consistency_threshold: ì¼ê´€ì„± ì„ê³„ê°’ (ê¸°ë³¸ 4/5 = 80%)
            source_rate: ì›ë³¸ ìƒ˜í”Œë ˆì´íŠ¸ (ê¸°ë³¸ 48000)

        Returns:
            - "APPLIANCE_DETECTED": ê°€ì „ ì†ŒìŒ í™•ì¸
            - None: ì™¸ë¶€ ì†ŒìŒ ë˜ëŠ” ì¼ê´€ì„± ë¶€ì¡±
        """
        if len(buffer_of_chunks) != 5:
            print(f"âš ï¸ ì²­í¬ ê°œìˆ˜ ì˜¤ë¥˜: {len(buffer_of_chunks)} (5ê°œ í•„ìš”)")
            return None

        # 5ê°œ ì²­í¬ ê°ê° ì¶”ë¡ 
        predictions = []
        confidences = []

        for i, chunk in enumerate(buffer_of_chunks):
            pred_class, confidence = self.predict_from_bytes(chunk, source_rate)
            predictions.append(pred_class)
            confidences.append(confidence)
            print(f"  Chunk {i+1}: {CLASS_NAMES[pred_class]} ({confidence*100:.1f}%)")

        # ê°€ì¥ ë§ì´ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤
        unique, counts = np.unique(predictions, return_counts=True)
        most_common_class = int(unique[np.argmax(counts)])
        most_common_count = int(np.max(counts))

        print(f"ğŸ“Š ê²°ê³¼: {CLASS_NAMES[most_common_class]} ({most_common_count}/5 ì¼ì¹˜)")

        # "Others" í´ë˜ìŠ¤ë©´ ë¬´ì‹œ
        if most_common_class == 3:
            print("âŒ ì™¸ë¶€ ì†ŒìŒ (Others) - ë¬´ì‹œ")
            return None

        # ì¼ê´€ì„± ì²´í¬ (ê¸°ë³¸ 80% = 4/5)
        if most_common_count >= consistency_threshold:
            # target_classê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ í´ë˜ìŠ¤ë§Œ ê°ì§€
            if target_class is not None and most_common_class != target_class:
                print(f"âŒ ë‹¤ë¥¸ ê°€ì „ ì†ŒìŒ ({CLASS_NAMES[most_common_class]}) - ë¬´ì‹œ (target: {CLASS_NAMES[target_class] if target_class < len(CLASS_NAMES) else target_class})")
                return None
            print(f"âœ… ê°€ì „ ì†ŒìŒ í™•ì¸: {CLASS_NAMES[most_common_class]}")
            return "APPLIANCE_DETECTED"
        else:
            print(f"âŒ ì¼ê´€ì„± ë¶€ì¡± ({most_common_count}/5) - ë¬´ì‹œ")
            return None


# ================= ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© (ì„ íƒì‚¬í•­) =================
def load_wav_16k_mono(filename):
    """
    í…ŒìŠ¤íŠ¸ìš©: wav íŒŒì¼ ì½ê¸° (íŒŒì´ì¬ ê¸°ë³¸ wave ëª¨ë“ˆ ì‚¬ìš©)
    ì‹¤ì œ ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨
    """
    import wave

    try:
        with wave.open(filename, 'rb') as wf:
            if wf.getframerate() != 16000:
                print(f"âš ï¸ {filename}ì€ 16kHzê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return np.zeros(INPUT_SIZE, dtype=np.float32)

            if wf.getnchannels() != 1:
                print(f"âš ï¸ {filename}ì€ Monoê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return np.zeros(INPUT_SIZE, dtype=np.float32)

            frames = wf.readframes(wf.getnframes())
            audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0

            if len(audio_data) < INPUT_SIZE:
                audio_data = np.pad(audio_data, (0, INPUT_SIZE - len(audio_data)))
            else:
                audio_data = audio_data[:INPUT_SIZE]

            return audio_data

    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return np.zeros(INPUT_SIZE, dtype=np.float32)


if __name__ == "__main__":
    print("ğŸ“¡ YAMNet í†µí•© ëª¨ë¸ í…ŒìŠ¤íŠ¸...")

    try:
        # í†µí•© ëª¨ë¸ ë¡œë“œ
        classifier = YAMNetClassifier()

        # í…ŒìŠ¤íŠ¸ 1: ë”ë¯¸ bytesë¡œ í…ŒìŠ¤íŠ¸ (48kHz)
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ 1: ë”ë¯¸ ì˜¤ë””ì˜¤ 48kHz (bytes) â†’ 16kHz ë¦¬ìƒ˜í”Œë§")
        dummy_audio_48k = np.random.randint(-32768, 32767, 48000, dtype=np.int16).tobytes()
        pred_class, confidence = classifier.predict_from_bytes(dummy_audio_48k, source_rate=48000)
        print(f"   ê²°ê³¼: {CLASS_NAMES[pred_class]} ({confidence*100:.1f}%)")

        # í…ŒìŠ¤íŠ¸ 2: 5ê°œ ì²­í¬ ë²„í¼
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ 2: 5ê°œ ì²­í¬ ë²„í¼ (48kHz)")
        buffer = [dummy_audio_48k] * 5
        result = classifier.classify_buffer(buffer, source_rate=48000)
        print(f"   ìµœì¢… ê²°ê³¼: {result}")

        # í…ŒìŠ¤íŠ¸ 3: wav íŒŒì¼ (ìˆìœ¼ë©´)
        test_file = "test.wav"
        if os.path.exists(test_file):
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ 3: {test_file}")
            audio_float = load_wav_16k_mono(test_file)
            # float32ë¥¼ int16 bytesë¡œ ë³€í™˜
            audio_bytes = (audio_float * 32768).astype(np.int16).tobytes()
            pred_class, confidence = classifier.predict_from_bytes(audio_bytes, source_rate=16000)
            print(f"   ê²°ê³¼: {CLASS_NAMES[pred_class]} ({confidence*100:.1f}%)")

        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ğŸ“ í†µí•© ëª¨ë¸ (YAMNet + Classifier) ì •ìƒ ì‘ë™")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
